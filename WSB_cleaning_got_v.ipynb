{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876d6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Obama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01d186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Obama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76f025f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Obama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d6b39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below code we are importing the data from where it is stored in the state of a csv file.\n",
    "# We will be chunking the data, this makes it easier on my computer and avoids memory errors\n",
    "# We will also be processing this data 1.5 million rows at a time, as you may notice by looking at the\n",
    "# nrows=1500000 in our pd.read_csv() command. \n",
    "# This is done to avoid the memory limitations of ram on my laptop. We will preprocess the data 1 million rows at a time\n",
    "# save those rows to a file, rinse and repeat for the total 34 million rows until the whole dataset is cleaned. \n",
    "# from there we will concatenate the data to one data frame and train our model on it.\n",
    "# , skiprows=range(1, 32000000)\n",
    "\n",
    "chunk_size = 200000\n",
    "df_chunks = pd.read_csv(\"C:/Users/pathway/.csv\", chunksize=chunk_size, encoding=\"ISO-8859-1\",skiprows=range(1, 29000001), nrows=1500000)\n",
    "df_fm = pd.concat(df_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset, lots of the columns are misaligned, after about every 500k or 1 million rows the columns  \n",
    "# shift a couple positions to the left or right. This happened because the author who got the data got it from the\n",
    "# Reddit API and called the data 500k or 1 million rows at a time; he then concatenated the whole dataset into a unclean \n",
    "# csv file.\n",
    "\n",
    "# To resolve this issue I will manually go through the dataset and inspect the columns, then take note of the shifts and \n",
    "# remedy them.\n",
    "\n",
    "# final note, we are still looking at the data 1.5 million rows at a time.\n",
    "\n",
    "# First, inspecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47966054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are changing some of the display settings for Jupyter Notebook so we can get a look at our data.\n",
    "# Using the .head() method we output the data to be seen.\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option(\"display.max_rows\", 1000000)\n",
    "pd.set_option(\"display.max_columns\", 55)\n",
    "df_fm.head(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = df_fm.iloc[499998:501050]\n",
    "\n",
    "ll.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra = df_fm.iloc[999980:1000100]\n",
    "intra.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "inj = df_fm.iloc[1200000:1200120]\n",
    "inj.head(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10771ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "las = df_fm[1499960:1500000]\n",
    "las.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rinse and repeat till the end of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the notes on the dataset I took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment p: So in the original dataset the columns are good until row 2 million, at row 2 million and 1, the dataset is shifted to\n",
    "# the right by one column at the \"created_utc\" column. To emphasize, \"created_utc\" at 2m1 is NaN and the real values are\n",
    "# one to the right at gildings.\n",
    "# close comment p.\n",
    "# \n",
    "# comment o: At 5,000,000 the pattern stops, at 5,000,001 it reverts back to the original 0-2m pattern and ends at 6 m \n",
    "# close comment 0.\n",
    "# \n",
    "# comment i: At 6m1 the rows become unstable but usable, they are shifted around this pattern continues until 7m.\n",
    "# close comment i.\n",
    "# \n",
    "# comment j: At 7m1 it time is under gildings, and body is under body until  8m\n",
    "# close comment j.\n",
    "# \n",
    "# comment bb: From 8m to 11m it is body under body time under created_utc, (in other words, normal).\n",
    "# close comment bb.\n",
    "#\n",
    "# comment k: starting at 11m to 12m body is under author_flair_type and time is under author_premium.\n",
    "# close comment k.\n",
    "#\n",
    "# comment h: starting at 12m1 to 14m body is under body and time is under gildings.\n",
    "# close comment h.\n",
    "#\n",
    "# comment g: staring at 14m to 16m body is under body and time is under created_utc.\n",
    "# close comment g.\n",
    "#\n",
    "# comment tt: starting at 16m to 18m body is inder body and time is under gildings.\n",
    "# close comment tt.\n",
    "#\n",
    "# comment pp: starting at 18m to 20m body is under body and time is under created_utc.\n",
    "# close comment pp.\n",
    "#\n",
    "# comment hh: starting at 20m to 21m the body is under body and time is under gildings.\n",
    "# close comment hh.\n",
    "#\n",
    "# comment qi: starting at 21m to 22m body under body and time under created_utc.\n",
    "# close comment qi.\n",
    "#\n",
    "# comment nn: starting at 22m to 24m body under body and time under gildings.\n",
    "# close comment nn.\n",
    "# \n",
    "\n",
    "# standard pull df_chunks = pd.read_csv(\"C:/Users/Obama/Downloads/kaggle_datasets/Reddit_wsb_data/wsb_comments_raw.csv\", chunksize=chunk_size, encoding=\"ISO-8859-1\", skiprows=range(1, 24000000), nrows=1500001)\n",
    "# error at ParserError: Error tokenizing data. C error: Expected 40 fields in line 25000002, saw 41\n",
    "# solution, will skip data from 24,999,990 to 25,000,010.\n",
    "# \n",
    "# \n",
    "# 24m-24.999,990 has body under author_flair_type and time under author_premium\n",
    "#\n",
    "# 25.000,010 to 26m has body under author_flair_text_color and time is under author_patreon_flair (authorname under associated_award)\n",
    "# \n",
    "# 26m to 28m body under author_premium and time under awarders (author still under associated_awards)\n",
    "# \n",
    "# \n",
    "# ##############(instance of misalignment) 26.5m to 28m   body u awarders time u body (author u author)##########################\n",
    "#\n",
    "#\n",
    "# ############# more errors ParserError: Error tokenizing data. C error: Expected 40 fields in line 29000002, saw 52 #####\n",
    "# \n",
    "# ###### will do skiprows=range(1, 28000000), nrows=1000001)\n",
    "#\n",
    "# \n",
    "# regardless of the data mess above. at 28m1 to 29m the data follows body under body time under created_utc and author under author.\n",
    "#\n",
    "#\n",
    "# at 29m10 to 30m body under author time under author_flair_css_class AUTHOR IS LOST\n",
    "#\n",
    "# at 30m to end ALL DATA IS LOST\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# \n",
    "# \n",
    "# 0-2million it is correct. 2m1-5m that the pattern listed in comment p holds up. 5,000,001 it goes to the original pattern (comment o documents this).\n",
    "# so far to 6 m the original pattern is good. At 6 million and 1 the pattern follows comment i until 7m. At 7m1 it follows the\n",
    "# pattern described in comment j until 8m. At 8m to 11m it follows comment bb. Starting at 11m to 12m it follows comment k.\n",
    "# At 12m1 the pattern follows comment h to 14m. starting at 14m to 16m comment g. starting at 16m to 18 comment tt. starting\n",
    "# at 18m to 20m comment pp. starting at comment 20m to 21m comment hh. starting at 21m to 22m comment qi. from 22m to 24m? comment\n",
    "# nn.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once I went through the dataset I saw two things; the data is usable and the ways the data is misaligned. With this known\n",
    "# I again opened the datasets 1.5 million rows at time, and then drop the unnecessary columns,\n",
    "# and finally I would manually rename the columns with their proper names. \n",
    "\n",
    "#(below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 200000\n",
    "df_chunks = pd.read_csv(\"C:/Users/pathway/.csv\", chunksize=chunk_size, encoding=\"ISO-8859-1\",skiprows=range(1, 29000001), nrows=1500000)\n",
    "df_fm = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "columns_to_drop = [\"author_cakeday\", \"edited\", \"media_metadata\", \"comment_type\", \"author_flair_background_color\",\n",
    "                   \"author_flair_css_class\", \"author_flair_richtext\", \"author_flair_template_id\", \"author_flair_text\", \n",
    "                  \"author_flair_text_color\", \"author_flair_type\", \"author_patreon_flair\", \"author_premium\", \"awarders\",\n",
    "                   \"collapsed_because_crowd_control\", \"gildings\", \"id\", \"is_submitter\", \"link_id\", \"locked\", \"no_follow\",\n",
    "                   \"parent_id\", \"permalink\", \"score\", \"send_replies\", \"stickied\", \"subreddit_id\", \"treatment_tags\", \"distinguished\",\n",
    "                   \"editable\", \"retrieved_on\", \"top_awarded_type\", \"subreddit\", \"associated_award\", \"all_awardings\"]\n",
    "\n",
    "uhoh = df_fm.drop(columns=columns_to_drop)\n",
    "\n",
    "new_names = {\"gildings\":\"created_utc\", \"author_flair\":\"body\"}\n",
    "\n",
    "realinged_df = uhoh.rename(columns=new_names)\n",
    "\n",
    "#\n",
    "# FYI on each different loaded 1.5 million rows, the step above would be different!!!!!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would then save the cleaner df to a new file for further cleaning, like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/pathway/\"\n",
    "file_name = \"WSB7-7.5M.csv\"\n",
    "\n",
    "file_path = folder_path + file_name\n",
    "\n",
    "realinged_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d166268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the dataset's data is under the right columns we can clean it for machine learning\n",
    "# Also, doing this manually took a long time :( , so I tried making it more automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below code we are:\n",
    "\n",
    "# deleting NaN and bad data\n",
    "\n",
    "# removing special characters\n",
    "\n",
    "# converting to lower case\n",
    "\n",
    "# removing extra whitespace\n",
    "\n",
    "# removing stop words\n",
    "\n",
    "# lemmatizing words\n",
    "\n",
    "# tokenizing the text\n",
    "\n",
    "# dropping data that is not relevant for topic modling, ie comments like \"yes\", \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aa6d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"WSB999,999-1.5M\", \"WSB1.5-2M\", \"WSB2-3M\", \"WSB3-4.5M\", \"WSB4.5-5M\", \"WSB5-6M\", \"WSB6-7M\", \"WSB7-7.5M\", \"WSB7.5-8M\", \"WSB8-9M\", \"WSB9-10M\", \"WSB10-10.5M\", \"WSB10.5-11M\", \"WSB11-12M\", \"WSB12-13.5M\", \"WSB13.5-14M\", \"WSB14-15M\", \"WSB15-16M\", \"WSB16-16.5M\", \"WSB16.5-18M\", \"WSB18-19M\", \"WSB19-19.5M\", \"WB19.5-20M\", \"WSB20-21M\", \"WSB21-22M\", \"WSB22-22.5M\", \"WSB22.5-24M\", \"WSB24-24.999,990M\", \"WSB25.000.010-26M\", \"WSB26-27.5M\", \"WSB27.5-28M\", \"WSB28-29M\"]\n",
    "for file in files:\n",
    "    df_nks = pd.read_csv(f\"C:/Users/pathway/{file}.csv\")\n",
    "    df = pd.DataFrame(df_nks)\n",
    "\n",
    "    def remove_special_characters(text):\n",
    "        if isinstance(text, str):  # Check if input is a string\n",
    "            cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "            return cleaned_text\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def convert_to_lowercase(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_extra_whitespace(text):\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        if isinstance(text, str) and not pd.isnull(text):\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            filtered_text = ' '.join([word for word in tokens if word.lower() not in stop_words])\n",
    "            return filtered_text\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    def lemmatize_words(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "        return lemmatized_text\n",
    "\n",
    "    df = df.drop(df[(df['body'].isna())].index)\n",
    "\n",
    "    df['body'] = df['body'].apply(remove_special_characters)\n",
    "\n",
    "    df['body'] = df['body'].apply(convert_to_lowercase)\n",
    "\n",
    "    df['body'] = df['body'].apply(remove_extra_whitespace)\n",
    "\n",
    "    df['body'] = df['body'].apply(remove_stopwords)\n",
    "\n",
    "    df['body'] = df['body'].apply(lemmatize_words)\n",
    "\n",
    "    words_delete = [\"removed\", \"deleted\", \" \", \"  \", \"   \"]\n",
    "    author_fullname = pd.NA\n",
    "\n",
    "\n",
    "    df = df.drop(df[(df['body'].isin(words_delete)) | (df['author_fullname'].isna())].index)\n",
    "    df = df.drop(df[df[\"author_fullname\"].isin(words_delete)].index)\n",
    "    df = df.drop(df[(df['created_utc'].isin(words_delete)) | (df['created_utc'].isna())].index)\n",
    "    df = df.drop(df[(df['author'].isin(words_delete)) | (df['author'].isna())].index)\n",
    "    df = df.drop(df[(df['total_awards_received'].isin(words_delete)) | (df['total_awards_received'].isna())].index)\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "    df['body'] = df['body'].apply(lambda x: tokenize_text(x))\n",
    "\n",
    "    useless = [\"\", \"yes\", \"ban\", \"way\", \"lol\", \"f\", \"nice\", \"fuck\", \"link\", \"ok\", \"know\",\n",
    "               \"rip\", \"thanks\", \"gay\", \"nope\", \"hope\", \"guh\", \"retarded\", \"stonks, go\", \"retard\", \"position, ban\",\n",
    "               \"eat, dongus, fuckin, nerd, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\", \n",
    "              \"buy\", \"fact\", \"right\", \n",
    "               \"post, flaired, dd, dd, list, find, fresh, wsb, dd, http, n, reddit, com, r, wallstreetbets, search, sort, new, amp, q, flair, 3add, amp, restrict, sr, amp, da, misuse, dd, flair, shitposts, short, vague, guess, unexplained, news, link, etc, please, change, flair, dd, mod, notified, thread, sure, flair, use, check, guide, post, flair, http, www, reddit, com, r, wallstreetbets, wiki, linkflair, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\"\n",
    "              \"good\", \"15, monday\", \"future\", \"yep\", \"next, week\", \"always\", \"say, take, see, hold, gain, nobody, left\", \"nah\", \"bruh\", \n",
    "              \"oof\", \"real\", \"please, resubmit, shorter, title, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\", \n",
    "              \"true\", \"thank\", \"rh\", \"bb\", \"say\", \"earnings\", \"go\", \"like\", \"broken, spoke, flair, plz, mod\", \"priced\", \"tldr\", \"yessir\", \n",
    "              \"maybe\", \"sir, bread, line, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\",\n",
    "              \"username, check\", \"exactly\", \"lmao\", \"son, bitch\", \"b\", \"lolol, future, barely, green\"]\n",
    "\n",
    "    df = df.drop(df[(df['body'].isin(useless))].index)\n",
    "\n",
    "    final_df = df.drop(\"author_fullname\", axis=1)\n",
    "    \n",
    "    folder_path = \"C:/Users/pathway/\"\n",
    "    file_name = file\n",
    "    csv_postfix = \".csv\"\n",
    "    file_path = folder_path + file_name + csv_postfix\n",
    "    final_df.to_csv(file_path, index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
